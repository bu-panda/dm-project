{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himani\\Anaconda2\\lib\\site-packages\\pandas\\io\\data.py:33: FutureWarning: \n",
      "The pandas.io.data module is moved to a separate package (pandas-datareader) and will be removed from pandas in a future version.\n",
      "After installing the pandas-datareader package (https://github.com/pydata/pandas-datareader), you can change the import ``from pandas.io import data, wb`` to ``from pandas_datareader import data, wb``.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#import sys\n",
    "#sys.path = ['', '/Users/panda/miniconda2/lib/python27.zip',\n",
    "#            '/Users/panda/miniconda2/lib/python2.7', '/Users/panda/miniconda2/lib/python2.7/plat-darwin', \n",
    "#            '/Users/panda/miniconda2/lib/python2.7/plat-mac', \n",
    "#            '/Users/panda/miniconda2/lib/python2.7/plat-mac/lib-scriptpackages', \n",
    "#            '/Users/panda/miniconda2/lib/python2.7/lib-tk', '/Users/panda/miniconda2/lib/python2.7/lib-old', \n",
    "#            '/Users/panda/miniconda2/lib/python2.7/lib-dynload', '/Users/panda/miniconda2/lib/python2.7/site-packages', \n",
    "#            '/Users/panda/miniconda2/lib/python2.7/site-packages/setuptools-19.6.2-py2.7.egg']\n",
    "\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.io import data\n",
    "from pandas import Series, DataFrame\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import scipy.sparse.linalg as linalg\n",
    "import scipy.cluster.hierarchy as hr\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.svm as svm\n",
    "import sklearn.cross_validation as cross_validation\n",
    "import sklearn.cluster as cluster\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import sklearn.datasets as sk_data\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import mixture\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.utils as utils\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from patsy import dmatrices\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "    \n",
    "import gmplot\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Station\n",
    "This is the class to aggregate data. Latter we will use its location information to draw our results on map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Cluster is based on each station's information, so this class is the abstract of station.\n",
    "\n",
    "'''\n",
    "class Station(object):\n",
    "    \n",
    "    # @station_id int\n",
    "    # Use an integer number to initilize an station instance\n",
    "    def __init__(self, station_id):\n",
    "        self.station_id = station_id\n",
    "        self.features = ['TEMP', 'DEWP', 'SLP', 'STP',\n",
    "                         'VISIB','WDSP','MXSPD',\n",
    "                         'GUST','MAX','MIN','SNDP','FOG',\n",
    "                        'RAIN','SNOW','HAIL','THUNDER','TORNADO']\n",
    "        \n",
    "        self.missingValue = [9999.9, 9999.9, 9999.9, 9999.9,\n",
    "                            999.9, 999.9, 999.9,999.9,\n",
    "                             9999.9, 9999.9,999.9,\n",
    "                            -1,-1,-1,-1,-1,-1]\n",
    "        \n",
    "        self.META = {}\n",
    "        self.data = {}\n",
    "        self.location = [0.0,0.0]\n",
    "        \n",
    "        for x in xrange(1,13,1):\n",
    "            self.META[x] = {}\n",
    "            self.data[x] = {}\n",
    "            for name in self.features:\n",
    "            \n",
    "                self.META[x][name] = 0\n",
    "                self.data[x][name] = 0\n",
    "    \n",
    "        \n",
    "    \n",
    "    # @month int\n",
    "    # @data dict, must include all the fields that list in self.features\n",
    "    # This method will filt all the default or missing value\n",
    "    def inputData(self, month, data):\n",
    "        \n",
    "        for i in xrange(len(self.features)):\n",
    "            name = self.features[i]\n",
    "            \n",
    "            rs = data.get(name, None)\n",
    "            \n",
    "            if rs != None and rs != self.missingValue[i] :\n",
    "                self.data[month][name] += rs\n",
    "                self.META[month][name] += 1\n",
    "    \n",
    "    # @month int\n",
    "    # return the summary for each month. This is an inner function\n",
    "    def summaryByMonth(self, month):\n",
    "        exp = ['FOG','RAIN','SNOW','HAIL','THUNDER','TORNADO']\n",
    "        result = []\n",
    "        for kind in self.features:\n",
    "            \n",
    "            if kind in exp:\n",
    "                result.append(self.data[month][kind])\n",
    "            else:\n",
    "                v = 0.0\n",
    "                if self.META[month][kind] != 0:\n",
    "                    v = (self.data[month][kind]+0.0) / (self.META[month][kind])\n",
    "                result.append(v)\n",
    "        return result\n",
    "                    \n",
    "    \n",
    "    # For each station, this method return the summary.\n",
    "    # This method return the feature vector for analyzing\n",
    "    def summaryFeatureVector(self):\n",
    "        rs = []\n",
    "        for x in xrange(1,13,1):\n",
    "            rs.extend(self.summaryByMonth(x))\n",
    "        return rs\n",
    "    \n",
    "    # This method return method dictinary\n",
    "    def summaryByDict(self):\n",
    "        rs = {}\n",
    "        for x in xrange(1,13,1):\n",
    "            rs[x] = self.summaryByMonth(x)\n",
    "        return rs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Some of the data may contains one bit flag like '*,A,I' and so on\n",
    "# This method will pop this value if it contains one bit tag\n",
    "def extractNumber(s):\n",
    "    last = s[-1]\n",
    "    l = len(s)\n",
    "    if ord(last) >= ord('0') and ord(last) <= ord('9'):\n",
    "        return s\n",
    "    else:\n",
    "        return s[0:l-1]\n",
    "\n",
    "# 6 special weather are represented by 6-bits string in the raw data\n",
    "# Here I extract them and write the value to its coresponded field  \n",
    "def extractSpecialWeather(d, s):\n",
    "    w = ['FOG', 'RAIN', 'SNOW', 'HAIL', 'THUNDER', 'TORNADO']\n",
    "    if len(s) != 6 :\n",
    "        print 'alert! not well-formed data'\n",
    "    \n",
    "    for i in xrange(6):\n",
    "        if s[i] == '1':\n",
    "            d[w[i]] = 1\n",
    "        else:\n",
    "            d[w[i]] = 0\n",
    "    return d\n",
    "    \n",
    "# Input the joined data file path\n",
    "# Read line by line and build a dict with all stations\n",
    "def buildStationDict(filepath):\n",
    "    rs = {}\n",
    "    f = open(filepath)\n",
    "    names = ['TEMP', 'DEWP', 'SLP', 'STP', 'VISIB', 'WDSP', 'MXSPD', \n",
    "             'GUST', 'MAX', 'MIN', 'SNDP', 'FOG', 'RAIN', 'SNOW', 'HAIL', \n",
    "             'THUNDER', 'TORNADO']\n",
    "    for each in f:\n",
    "        data = each.strip().split(',')\n",
    "        station_id = int(data[0])\n",
    "        month = data[2][4:6]\n",
    "        month = int(month)\n",
    "        fdict = {}\n",
    "        i = 0\n",
    "        for x in xrange(3, 17, 2):\n",
    "            fdict[names[i]] = float(data[x])\n",
    "            i+=1\n",
    "        fdict['GUST'] = float(data[16])\n",
    "        \n",
    "        maxV = extractNumber(data[17])\n",
    "        minV = extractNumber(data[18])\n",
    "        \n",
    "        \n",
    "        fdict['MAX'] = float(maxV)\n",
    "        fdict['MIN'] = float(minV)\n",
    "        fdict['SNDP'] = float(data[20])\n",
    "        fdict = extractSpecialWeather(fdict, data[21])\n",
    "        \n",
    "        if rs.get(station_id) == None:\n",
    "            p = Station(station_id)\n",
    "            rs[station_id] = p\n",
    "        p = rs[station_id]\n",
    "        p.location = [data[-2],data[-1]]\n",
    "        p.inputData(month, fdict)\n",
    "    return rs\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "# Convert raw data location format\n",
    "# For the latitude and logitude, E,N will be positive while W,S will be negative\n",
    "def getLocation(loc):\n",
    "    rs = [0,0]\n",
    "    \n",
    "    la = loc[0]\n",
    "    lo = loc[1]\n",
    "    \n",
    "    end_la = la[-1]\n",
    "    end_lo = lo[-1]\n",
    "    \n",
    "    l1 = len(la)\n",
    "    l2 = len(lo)\n",
    "    \n",
    "    la = la[0:l1-1]\n",
    "    lo = lo[0: l2-1]\n",
    "    \n",
    "    if end_la == 'N':\n",
    "        rs[0] = float(la) / 100.0\n",
    "    else:\n",
    "        rs[0] = float(la) / -100.0\n",
    "    \n",
    "    if end_lo == 'E':\n",
    "        rs[1] = float(lo) / 100.0\n",
    "    else:\n",
    "        rs[1] = float(lo) / -100.0\n",
    "    return rs    \n",
    "    \n",
    "# @data pandas.DataFrame  Must contains 3 feilds, 'latitude','longitude','label'\n",
    "# @filepath str specify where you want to store the result\n",
    "# @colors, If you have more than 6 labels, you should specify your own color list\n",
    "def showOnMap(data, filepath,colors = []):\n",
    "    gmap = gmplot.GoogleMapPlotter(36.161517,-115.164011,16)\n",
    "\n",
    "    la = list(data['latitude'])\n",
    "    lo = list(data['longitude'])\n",
    "    label = list(data['label'])\n",
    "    \n",
    "    if len(colors) == 0:\n",
    "        colors = ['#ff0000','#00ff00','#0000ff','#ffff00','#ff00ff','#00ffff']\n",
    "    draw_la = collections.defaultdict(list)\n",
    "    draw_lo = collections.defaultdict(list)\n",
    "\n",
    "    kind = len(colors)\n",
    "    for i in range(len(label)):\n",
    "        try:\n",
    "            color = colors[int(label[i]) % kind]\n",
    "        \n",
    "            draw_la[color].append(la[i])\n",
    "            draw_lo[color].append(lo[i])\n",
    "        except:\n",
    "            print i\n",
    "            print label[i]\n",
    "\n",
    "    for each in draw_la.keys():\n",
    "        gmap.scatter(draw_la[each], draw_lo[each], each, size=10000, marker=False)\n",
    "    gmap.draw(filepath)   \n",
    "\n",
    "    \n",
    "# @data pandas.DataFrame\n",
    "# @n_clusters  How many clusters you want\n",
    "# @plot_graph Only to be True if you want to plot the graph, especially when there are only 2 labels\n",
    "def GMMCluster(data, n_clusters = 3, plot_graph = False):\n",
    "    colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])\n",
    "    print n_clusters\n",
    "    gmm = mixture.GMM(n_components=n_clusters, covariance_type='diag')\n",
    "    gmm.fit(data)\n",
    "    y_pred = gmm.predict(data)\n",
    "    \n",
    "    if plot_graph:   \n",
    "        plt.scatter(data[:, 0], data[:, 1], s=10,color=colors[y_pred].tolist(), alpha=0.8)\n",
    "        plt.show()\n",
    "    return y_pred\n",
    "\n",
    "'''\n",
    "data: ndarray. Data matrix\n",
    "n_clusters: int. How many clusters you want to have.\n",
    "detail: bool. Whether to out put some detail information\n",
    "plot_graph: bool. Whether to plot heat graph to compare unsorted and sorted label. \n",
    "\n",
    "return type (labels, centers)\n",
    "'''\n",
    "def kMeansCluster(data,clusters = 10, detail = False,plot_graph = False):\n",
    "    colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=clusters, n_init=10)\n",
    "    kmeans.fit_predict(data)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    labels = kmeans.labels_\n",
    "    error = kmeans.inertia_\n",
    "    \n",
    "    if detail:\n",
    "        print \"The total error of the clustering is: \", error\n",
    "        print '\\nCluster labels'\n",
    "        print labels\n",
    "        print '\\n Cluster Centroids'\n",
    "        print centroids\n",
    "        \n",
    "    if plot_graph:\n",
    "        \n",
    "        plt.scatter(data[:, 0], data[:, 1], s=10,color=colors[labels].tolist(), alpha=0.8)\n",
    "        plt.show()\n",
    "        #idx = np.argsort(labels)\n",
    "        #rX = X[idx,:]\n",
    "        #fig, (ax1, ax2) = plt.subplots(1,2,figsize=(15,10))\n",
    "        #sns.heatmap( rX,xticklabels=False, yticklabels=False, linewidths=0,cbar=True,ax = ax1)\n",
    "        #sns.heatmap( X,xticklabels=False, yticklabels=False, linewidths=0,cbar=True,ax = ax2)\n",
    "    return (labels, centroids)\n",
    "\n",
    "'''\n",
    "data: ndarry. Data matrix\n",
    "begin: int. The beginning test number of cluster\n",
    "end: int. The maximum number of clusters\n",
    "'''\n",
    "def estimateCluster(data, end = 11):\n",
    "    error = np.zeros(end)\n",
    "    error[0] = 0;\n",
    "    for k in range(1,end):\n",
    "        kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10)\n",
    "        kmeans.fit_predict(data)\n",
    "        error[k] = kmeans.inertia_\n",
    "\n",
    "    plt.plot(range(1,len(error)),error[1:])\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-e2a26f7d4215>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfdict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'fdict' is not defined"
     ]
    }
   ],
   "source": [
    "fdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the station dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = buildStationDict('data_with_location.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert station to feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = []\n",
    "locations = []    \n",
    "\n",
    "for station_id in m.keys():    \n",
    "    station = m[station_id]\n",
    "    locations.append( getLocation( station.location) )\n",
    "    features.append(station.summaryFeatureVector())\n",
    "    \n",
    "\n",
    "data = np.array(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ec = estimateCluster(data, 11)\n",
    "ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 ..., 0 0 2]\n",
      "[[  3.57550854e+01   2.68108735e+01   1.01928453e+03 ...,   5.90142672e-02\n",
      "    6.75421530e-01   2.26977951e-03]\n",
      " [  3.49423147e+01   2.64803912e+01  -1.06865627e-11 ...,   3.49544073e-02\n",
      "    6.28419453e-01   7.59878419e-03]\n",
      " [  4.66175805e+01   3.48428485e+01   9.82126305e+02 ...,   1.90288714e-02\n",
      "    5.67585302e-01   1.96850394e-03]\n",
      " ..., \n",
      " [  4.45968033e+01   3.73102191e+01   1.01865005e+03 ...,   7.21153846e-02\n",
      "    5.48076923e-01   9.61538462e-03]\n",
      " [  4.71118729e+01   3.72591446e+01   9.76190305e+02 ...,   1.58620690e-01\n",
      "    1.45517241e+00  -6.93889390e-18]\n",
      " [  1.95578063e+01   1.28512261e+01   8.71576079e+01 ...,   1.72413793e-02\n",
      "    7.58620690e-01  -2.60208521e-18]]\n"
     ]
    }
   ],
   "source": [
    "labels,centers = kMeansCluster(data,10)\n",
    "print labels\n",
    "print centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'label', u'latitude', u'longitude'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "las = []\n",
    "los = []\n",
    "label = []\n",
    "i = 0\n",
    "for each in locations:\n",
    "    las.append(each[0])\n",
    "    los.append(each[1])\n",
    "    label.append(labels[i])\n",
    "    i+=1\n",
    "\n",
    "rs = {'latitude':las, 'longitude':los, 'label':label}\n",
    "\n",
    "df = pd.DataFrame(rs)\n",
    "\n",
    "print df.columns\n",
    "\n",
    "colors = ['#000000','#0000FF','#8A2BE2',\n",
    "          '#5F9EA0','#7FFF00','#FF7F50',\n",
    "          '#DC143C', '#00FFFF','#006400',\n",
    "         '#8B008B']\n",
    "\n",
    "    \n",
    "\n",
    "showOnMap(df, 'cluster.html',colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do GMM cluster\n",
    "Do GMM cluster and show it on the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "labels = GMMCluster(data, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'label', u'latitude', u'longitude'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "las = []\n",
    "los = []\n",
    "label = []\n",
    "i = 0\n",
    "for each in locations:\n",
    "    las.append(each[0])\n",
    "    los.append(each[1])\n",
    "    label.append(labels[i])\n",
    "    i+=1\n",
    "\n",
    "rs = {'latitude':las, 'longitude':los, 'label':label}\n",
    "\n",
    "df = pd.DataFrame(rs)\n",
    "\n",
    "print df.columns\n",
    "\n",
    "colors = ['#000000','#0000FF','#8A2BE2',\n",
    "          '#5F9EA0','#7FFF00','#FF7F50',\n",
    "          '#DC143C', '#00FFFF','#006400',\n",
    "         '#8B008B', '#556B2F','#483D8B',\n",
    "         '#9400D3','#FFD700','#FF4500']\n",
    "\n",
    "    \n",
    "\n",
    "showOnMap(df, 'GMM-cluster.html',colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Next part SVM\n",
    "\n",
    "- First part is the helper functions\n",
    "- Use SVM to predict special weather\n",
    "- Draw ROC curve to show our result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# @weather_str 6-bits string. \n",
    "# @in_detail if True, return lables from 0 to 63. If False, return boolean labels\n",
    "def getSpecialWeatherLabel(weather_str, in_detail = True):\n",
    "    \n",
    "    label = 0\n",
    "    for x in range(6):\n",
    "        weight = 1 << x\n",
    "        v = int(weather_str[x])\n",
    "        label += weight*v\n",
    "    \n",
    "    if in_detail:\n",
    "        return label\n",
    "    else:\n",
    "        return label == 0\n",
    "        \n",
    "\n",
    "# For each row in the file, get its features and labels\n",
    "# @filepath input data file path\n",
    "# @return type: pandas.DataFrame\n",
    "def extractClassificationTable(filepath):\n",
    "    f = open(filepath)\n",
    "    \n",
    "    fdict = collections.defaultdict(list)\n",
    "    \n",
    "    names = ['TEMP', 'DEWP', 'SLP', 'STP', 'VISIB', 'WDSP', 'MXSPD', \n",
    "             'GUST', 'MAX', 'MIN', 'SNDP', 'FOG', 'RAIN', 'SNOW', 'HAIL', \n",
    "             'THUNDER', 'TORNADO']\n",
    "    for each in f:\n",
    "        data = each.strip().split(',')\n",
    "        #get month\n",
    "        month = data[2][4:6]\n",
    "        month = int(month)\n",
    "        \n",
    "        fdict['MONTH'].append(month)\n",
    "        \n",
    "        i = 0\n",
    "        for x in xrange(3, 17, 2):\n",
    "            fdict[names[i]].append(float(data[x]))\n",
    "            i+=1\n",
    "        fdict['GUST'].append(float(data[16]))\n",
    "        \n",
    "        maxV = extractNumber(data[17])\n",
    "        minV = extractNumber(data[18])\n",
    "        \n",
    "        \n",
    "        fdict['MAX'].append(float(maxV))\n",
    "        fdict['MIN'].append(float(minV))\n",
    "        fdict['SNDP'].append(float(data[20]))\n",
    "        fdict['LABEL'].append(getSpecialWeatherLabel(data[21], True))\n",
    "        \n",
    "        lati, logi = getLocation([data[-2],data[-1]])\n",
    "        fdict['LAT'].append(lati)\n",
    "        fdict['LOG'].append(logi)\n",
    "    \n",
    "    frame = pd.DataFrame(fdict)\n",
    "    \n",
    "    return frame\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MULTIPLE REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classify_dataframe = extractClassificationTable('data_narrow.csv')\n",
    "cf = classify_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3548.1247812\n"
     ]
    }
   ],
   "source": [
    "names = 'SLP'\n",
    "names = ['SLP']\n",
    "sum_values=0\n",
    "mean=0\n",
    "count=0\n",
    "#column_len = len(cf['SLP'])\n",
    "    #print column_len\n",
    "for i in range(0,len(names)):\n",
    "    column_len = cf[names]\n",
    "    for j in range(0,column_len):\n",
    "        value =  cf[names][j]\n",
    "    ##print cf[name][j]\n",
    "        if (value  != 999.9) or (value  != 9999.9) or (value  != -1):\n",
    "            mean = mean + value \n",
    "            count+=1\n",
    "    sum_values = (mean / count)\n",
    "    print sum_values\n",
    "#return sum_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mean_columns(name,cf):\n",
    "    sum_values=0\n",
    "    mean=0\n",
    "    column_len = len(cf[name])\n",
    "    count=0\n",
    "    #print name\n",
    "    #print name\n",
    "    for j in range(0,column_len):\n",
    "        value =  cf[name][j]\n",
    "        #print value\n",
    "        if (value  != 999.9) or (value  != 9999.9) or (value  != -1):\n",
    "            mean = mean + value\n",
    "            count+=1\n",
    "    if count!=0:\n",
    "        sum_values = (mean / count)\n",
    "    else:\n",
    "        sum_values=0\n",
    "    return sum_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def replace(sum_values,name,cf):\n",
    "        column_len = len(cf[name])\n",
    "        for j in range(0,column_len):\n",
    "            value =  cf[name][j]\n",
    "            #print value\n",
    "            if (value  == 999.9) or (value  != 9999.9) or (value  != -1):\n",
    "                cf[name][j] = sum_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLP\n",
      "3418.91302244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himani\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-140-b054389e2e51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0msum_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0msum_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-139-9d48ceca695e>\u001b[0m in \u001b[0;36mreplace\u001b[1;34m(sum_values, name, cf)\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[1;31m#print value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m  \u001b[1;33m==\u001b[0m \u001b[1;36m999.9\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m  \u001b[1;33m!=\u001b[0m \u001b[1;36m9999.9\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m  \u001b[1;33m!=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                 \u001b[0mcf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\himani\\Anaconda2\\lib\\site-packages\\pandas\\core\\series.pyc\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m         \u001b[1;31m# do the setitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 717\u001b[1;33m         \u001b[0mcacher_needs_updating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_is_chained_assignment_possible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    718\u001b[0m         \u001b[0msetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcacher_needs_updating\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\himani\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36m_check_is_chained_assignment_possible\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1229\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_view\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_cached\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1230\u001b[0m             \u001b[0mref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_cacher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1231\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mref\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_mixed_type\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1232\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_setitem_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'referant'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1233\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\himani\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36m_is_mixed_type\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2322\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_mixed_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2323\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_mixed_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2324\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2326\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\himani\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36m_protect_consolidate\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m   2286\u001b[0m         \u001b[1;34m\"\"\" consolidate _data. if the blocks have changed, then clear the cache \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2287\u001b[0m         \u001b[0mblocks_before\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2288\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2289\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mblocks_before\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2290\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\himani\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m   2321\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2322\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_mixed_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2323\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_mixed_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2324\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sum_values=0\n",
    "names = ['SLP', 'DEWP', 'GUST', 'STP', 'WDSP', 'MXSPD', \n",
    "             'TEMP', 'MAX', 'MIN', 'SNDP']\n",
    "for i in range(0,len(names)):\n",
    "    print names[i]\n",
    "    sum_values = mean_columns(names[i],cf)\n",
    "    print sum_values\n",
    "    replace(sum_values,names[i],cf)\n",
    "    if i ==1:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>LABEL</td>      <th>  R-squared:         </th> <td>   0.014</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.013</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   27.49</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 18 Apr 2016</td> <th>  Prob (F-statistic):</th> <td>7.17e-63</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:42:23</td>     <th>  Log-Likelihood:    </th> <td> -67424.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 23787</td>      <th>  AIC:               </th> <td>1.349e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 23774</td>      <th>  BIC:               </th> <td>1.350e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    12</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    2.0192</td> <td>    0.149</td> <td>   13.523</td> <td> 0.000</td> <td>    1.727     2.312</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DEWP</th>  <td>   -0.0001</td> <td> 1.79e-05</td> <td>   -6.933</td> <td> 0.000</td> <td>   -0.000 -8.89e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GUST</th>  <td>   -0.0005</td> <td> 6.96e-05</td> <td>   -7.580</td> <td> 0.000</td> <td>   -0.001    -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LAT</th>   <td>    0.0027</td> <td>    0.001</td> <td>    2.347</td> <td> 0.019</td> <td>    0.000     0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LOG</th>   <td>    0.0018</td> <td>    0.000</td> <td>    5.041</td> <td> 0.000</td> <td>    0.001     0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MAX</th>   <td>   -0.0002</td> <td>    0.000</td> <td>   -1.555</td> <td> 0.120</td> <td>   -0.000  4.36e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MIN</th>   <td>    0.0002</td> <td>    0.000</td> <td>    1.897</td> <td> 0.058</td> <td>-6.68e-06     0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MXSPD</th> <td>    0.0002</td> <td>    0.000</td> <td>    0.799</td> <td> 0.424</td> <td>   -0.000     0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SLP</th>   <td>-1.042e-05</td> <td> 7.41e-06</td> <td>   -1.407</td> <td> 0.159</td> <td>-2.49e-05   4.1e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SNDP</th>  <td>   -0.0011</td> <td>    0.000</td> <td>   -8.473</td> <td> 0.000</td> <td>   -0.001    -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>STP</th>   <td> 2.763e-05</td> <td> 7.21e-06</td> <td>    3.834</td> <td> 0.000</td> <td> 1.35e-05  4.18e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TEMP</th>  <td>    0.0181</td> <td>    0.001</td> <td>   12.861</td> <td> 0.000</td> <td>    0.015     0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>WDSP</th>  <td> 2.371e-05</td> <td>    0.000</td> <td>    0.065</td> <td> 0.949</td> <td>   -0.001     0.001</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>16498.347</td> <th>  Durbin-Watson:     </th>  <td>   1.763</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>191519.582</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 3.353</td>   <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>15.176</td>   <th>  Cond. No.          </th>  <td>4.63e+04</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  LABEL   R-squared:                       0.014\n",
       "Model:                            OLS   Adj. R-squared:                  0.013\n",
       "Method:                 Least Squares   F-statistic:                     27.49\n",
       "Date:                Mon, 18 Apr 2016   Prob (F-statistic):           7.17e-63\n",
       "Time:                        22:42:23   Log-Likelihood:                -67424.\n",
       "No. Observations:               23787   AIC:                         1.349e+05\n",
       "Df Residuals:                   23774   BIC:                         1.350e+05\n",
       "Df Model:                          12                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "const          2.0192      0.149     13.523      0.000         1.727     2.312\n",
       "DEWP          -0.0001   1.79e-05     -6.933      0.000        -0.000 -8.89e-05\n",
       "GUST          -0.0005   6.96e-05     -7.580      0.000        -0.001    -0.000\n",
       "LAT            0.0027      0.001      2.347      0.019         0.000     0.005\n",
       "LOG            0.0018      0.000      5.041      0.000         0.001     0.003\n",
       "MAX           -0.0002      0.000     -1.555      0.120        -0.000  4.36e-05\n",
       "MIN            0.0002      0.000      1.897      0.058     -6.68e-06     0.000\n",
       "MXSPD          0.0002      0.000      0.799      0.424        -0.000     0.001\n",
       "SLP        -1.042e-05   7.41e-06     -1.407      0.159     -2.49e-05   4.1e-06\n",
       "SNDP          -0.0011      0.000     -8.473      0.000        -0.001    -0.001\n",
       "STP         2.763e-05   7.21e-06      3.834      0.000      1.35e-05  4.18e-05\n",
       "TEMP           0.0181      0.001     12.861      0.000         0.015     0.021\n",
       "WDSP        2.371e-05      0.000      0.065      0.949        -0.001     0.001\n",
       "==============================================================================\n",
       "Omnibus:                    16498.347   Durbin-Watson:                   1.763\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           191519.582\n",
       "Skew:                           3.353   Prob(JB):                         0.00\n",
       "Kurtosis:                      15.176   Cond. No.                     4.63e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 4.63e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "y = cf['LABEL']\n",
    "X = cf[['DEWP', 'GUST', 'LAT', 'LOG', 'MAX', 'MIN', 'MXSPD', 'SLP', 'SNDP', 'STP','TEMP', 'WDSP']]\n",
    "X = sm.add_constant(X)\n",
    "model1 = sm.OLS(y, X).fit()\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<type 'numpy.ndarray'>\n",
      "(23787L, 10L)\n",
      "<class 'pandas.core.series.Series'>\n",
      "(23787L,)\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  LABEL   R-squared:                       0.150\n",
      "Model:                            OLS   Adj. R-squared:                  0.150\n",
      "Method:                 Least Squares   F-statistic:                     420.9\n",
      "Date:                Mon, 18 Apr 2016   Prob (F-statistic):               0.00\n",
      "Time:                        22:52:09   Log-Likelihood:                -67578.\n",
      "No. Observations:               23787   AIC:                         1.352e+05\n",
      "Df Residuals:                   23777   BIC:                         1.353e+05\n",
      "Df Model:                          10                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "x1            -0.0001   1.79e-05     -6.998      0.000        -0.000 -9.01e-05\n",
      "x2          1.068e-05   5.99e-05      0.178      0.858        -0.000     0.000\n",
      "x3            -0.0001      0.000     -1.354      0.176        -0.000  6.56e-05\n",
      "x4             0.0002      0.000      1.594      0.111     -3.88e-05     0.000\n",
      "x5             0.0005      0.000      1.822      0.068     -3.65e-05     0.001\n",
      "x6          1.495e-06   7.39e-06      0.202      0.840      -1.3e-05   1.6e-05\n",
      "x7             0.0004   9.41e-05      4.395      0.000         0.000     0.001\n",
      "x8          2.998e-05   6.74e-06      4.448      0.000      1.68e-05  4.32e-05\n",
      "x9             0.0199      0.001     15.038      0.000         0.017     0.023\n",
      "x10           -0.0004      0.000     -0.950      0.342        -0.001     0.000\n",
      "==============================================================================\n",
      "Omnibus:                    15981.401   Durbin-Watson:                   1.753\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           172538.313\n",
      "Skew:                           3.237   Prob(JB):                         0.00\n",
      "Kurtosis:                      14.496   Cond. No.                         409.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "Confidence Intervals:             0         1\n",
      "x1  -0.000160 -0.000090\n",
      "x2  -0.000107  0.000128\n",
      "x3  -0.000359  0.000066\n",
      "x4  -0.000039  0.000376\n",
      "x5  -0.000036  0.000998\n",
      "x6  -0.000013  0.000016\n",
      "x7   0.000229  0.000598\n",
      "x8   0.000017  0.000043\n",
      "x9   0.017335  0.022531\n",
      "x10 -0.001074  0.000373\n",
      "Parameters: x1    -0.000125\n",
      "x2     0.000011\n",
      "x3    -0.000147\n",
      "x4     0.000169\n",
      "x5     0.000481\n",
      "x6     0.000001\n",
      "x7     0.000414\n",
      "x8     0.000030\n",
      "x9     0.019933\n",
      "x10   -0.000351\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "#print type(df)\n",
    "#df = df.convert_objects(convert_numeric=True)\n",
    "print type(cf)\n",
    "X = cf[['DEWP', 'GUST', 'MAX', 'MIN', 'MXSPD', 'SLP', 'SNDP', 'STP','TEMP', 'WDSP']].values\n",
    "print type(X)\n",
    "print X.shape\n",
    "#print df.shape\n",
    "y = cf['LABEL']\n",
    "print type(y)\n",
    "#print y\n",
    "print y.shape\n",
    "#mod = smf.ols(formula='Average_Score ~ Location + Sleep + Rooms + Service + Value + Clean', data=df)\n",
    "#res = mod.fit()\n",
    "#print(res.summary())\n",
    "#print df\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print results.summary()\n",
    "print \"Confidence Intervals:\", results.conf_int()\n",
    "print \"Parameters:\", results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "y = weather_labels.values\n",
    "X = classify_dataframe.values\n",
    "\n",
    "X, y = utils.shuffle(X, y, random_state=1)\n",
    "print X.shape\n",
    "print y.shape\n",
    "print y\n",
    "train_set_size = 5000\n",
    "X_train = X[:train_set_size]  # selects first 10000 rows (examples) for train set\n",
    "y_train = y[:train_set_size]\n",
    "X_test = X[train_set_size:]   # selects from row 10000 until the last one for test set\n",
    "y_test = y[train_set_size:]\n",
    "print(X_train.shape), y_train.shape\n",
    "print(X_test.shape), y_test.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rf = RandomForestClassifier(n_estimators=50,oob_score=True)#number of trees in the forest is 50 , '100' gives just little more accuracy than '50'\n",
    "rf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "pred=rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rf.score(X_test,y_test)#mean accuracy 0f the given test data and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, pred[:])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print roc_auc  #roc_auc is the vealuation metric for random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "print sklearn.metrics.confusion_matrix(y_test,pred)# confusion matrix \n",
    "print sklearn.metrics.accuracy_score(y_test, pred)  # 76.5 % areas were classified corrrectly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "trees=range(50)\n",
    "accuracy=np.zeros(50)\n",
    "\n",
    "for idx in range(len(trees)):\n",
    "    classifier=RandomForestClassifier(n_estimators=idx + 1)\n",
    "    classifier=classifier.fit(X_train,y_train)\n",
    "    predictions=classifier.predict(X_test)\n",
    "    accuracy[idx]=sklearn.metrics.accuracy_score(y_test, pred)#accuracy score for each of random classifier trees from 1 to 50 is stored in an array\n",
    "plt.cla()\n",
    "plt.plot(trees, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "f = open('data_with_location.csv')\n",
    "fin = open('data_narrow.csv','w')\n",
    "\n",
    "for each in f:\n",
    "    i = random.random()\n",
    "    if i < 0.01:\n",
    "        fin.write(each)\n",
    "\n",
    "f.close()\n",
    "fin.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23787, 14)\n",
      "   DEWP   GUST    LAT  LOG   MAX   MIN  MONTH  MXSPD     SLP   SNDP     STP  \\\n",
      "0  10.6  999.9  70.56 -8.4  26.8  12.0      3   27.2  1030.1  999.9  1028.9   \n",
      "1  20.3  999.9  70.56 -8.4  30.6  23.4      4   23.3  1015.3  999.9  1014.1   \n",
      "\n",
      "   TEMP  VISIB  WDSP  \n",
      "0  17.1   19.7  21.9  \n",
      "1  27.3   24.4  12.9  \n",
      "Index([u'DEWP', u'GUST', u'LAT', u'LOG', u'MAX', u'MIN', u'MONTH', u'MXSPD',\n",
      "       u'SLP', u'SNDP', u'STP', u'TEMP', u'VISIB', u'WDSP'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "classify_dataframe = extractClassificationTable('data_narrow.csv')\n",
    "weather_labels  =classify_dataframe.pop('LABEL')\n",
    "print classify_dataframe.shape\n",
    "print classify_dataframe.head(2)\n",
    "print classify_dataframe.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset to train and test part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23787L, 14L)\n",
      "(23787L,)\n",
      "[False  True  True ...,  True  True  True]\n",
      "(5000L, 14L) (5000L,)\n",
      "(18787L, 14L) (18787L,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y = weather_labels.values\n",
    "X = classify_dataframe.values\n",
    "\n",
    "X, y = utils.shuffle(X, y, random_state=1)\n",
    "print X.shape\n",
    "print y.shape\n",
    "print y\n",
    "train_set_size = 5000\n",
    "X_train = X[:train_set_size]  # selects first 10000 rows (examples) for train set\n",
    "y_train = y[:train_set_size]\n",
    "X_test = X[train_set_size:]   # selects from row 10000 until the last one for test set\n",
    "y_test = y[train_set_size:]\n",
    "print(X_train.shape), y_train.shape\n",
    "print(X_test.shape), y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do SVM classification and show its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM test set: 0.561239154735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himani\\Anaconda2\\lib\\site-packages\\scikit_learn-0.17.1-py2.7-win-amd64.egg\\sklearn\\svm\\base.py:224: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "\n",
    "svc = svm.SVC(kernel='linear', max_iter=10000)\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred_test = svc.predict(X_test)\n",
    "print \"Accuracy of SVM test set:\", svc.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ROC curve to evaluate our result\n",
    "When you try to plot ROC curve, make sure your label is binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11894L, 1L)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,\n",
    "                                                    random_state=0)\n",
    "\n",
    "classifier = OneVsRestClassifier(svc)\n",
    "\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "print y_score.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(2):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:], y_score[:])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# Plot of a ROC curve for a specific class\n",
    "plt.figure()\n",
    "plt.plot(fpr[1], tpr[1], label='ROC curve (area = %0.2f)' % roc_auc[1])\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "RANDOM FOREST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23787L, 14L)\n",
      "(23787L,)\n",
      "[False  True  True ...,  True  True  True]\n",
      "(5000L, 14L) (5000L,)\n",
      "(18787L, 14L) (18787L,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "y = weather_labels.values\n",
    "X = classify_dataframe.values\n",
    "\n",
    "X, y = utils.shuffle(X, y, random_state=1)\n",
    "print X.shape\n",
    "print y.shape\n",
    "print y\n",
    "train_set_size = 5000\n",
    "X_train = X[:train_set_size]  # selects first 10000 rows (examples) for train set\n",
    "y_train = y[:train_set_size]\n",
    "X_test = X[train_set_size:]   # selects from row 10000 until the last one for test set\n",
    "y_test = y[train_set_size:]\n",
    "print(X_train.shape), y_train.shape\n",
    "print(X_test.shape), y_test.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "            oob_score=True, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=50,oob_score=True)#number of trees in the forest is 50 , '100' gives just little more accuracy than '50'\n",
    "rf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred=rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76020652578910952"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_test,y_test)#mean accuracy 0f the given test data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.709112631798\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, pred[:])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print roc_auc  #roc_auc is the vealuation metric for random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3545  3046]\n",
      " [ 1459 10737]]\n",
      "0.760206525789\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "print sklearn.metrics.confusion_matrix(y_test,pred)# confusion matrix \n",
    "print sklearn.metrics.accuracy_score(y_test, pred)  # 76.5 % areas were classified corrrectly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1abc44e0>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "trees=range(50)\n",
    "accuracy=np.zeros(50)\n",
    "\n",
    "for idx in range(len(trees)):\n",
    "    classifier=RandomForestClassifier(n_estimators=idx + 1)\n",
    "    classifier=classifier.fit(X_train,y_train)\n",
    "    predictions=classifier.predict(X_test)\n",
    "    accuracy[idx]=sklearn.metrics.accuracy_score(y_test, pred)#accuracy score for each of random classifier trees from 1 to 50 is stored in an array\n",
    "plt.cla()\n",
    "plt.plot(trees, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
